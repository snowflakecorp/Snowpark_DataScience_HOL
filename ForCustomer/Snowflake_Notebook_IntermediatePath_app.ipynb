{"metadata":{"hex_info":{"author":"Diana Shaw","exported_date":"Thu Feb 29 2024 20:20:21 GMT+0000 (Coordinated Universal Time)","project_id":"b4861126-fb2c-4f2e-a1db-f725fca1baba","version":"draft"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"name":"HOL_Overview","collapsed":false},"source":"# Overview: Flow of Snowpark ML API / MLOps Hands-on-Lab\nThis \"Intermediate Path\" notebook is recommended for Python-savvy attendees. Users will need to complete missing code and add critical variables in order for cells to run.","id":"276544ab-e258-4602-a809-f3db660dba95"},{"cell_type":"markdown","metadata":{"name":"HOL_Background","collapsed":false},"source":["## Background Information\n","\n","Tasty Bytes is one of the largest food truck networks in the world with localized menu options spread across 30 major cities in 15 countries. **Tasty Bytes is aiming to achieve 25% YoY sales growth over 5 years.**\n","\n","As Tasty Bytes Data Scientists, we have been asked to support this goal by helping our food truck drivers more intelligently pick where to park for shifts. \n","\n","**We want to direct our trucks to locations that are expected to have the highest sales on a given shift.\n","This will maximize our daily revenue across our fleet of trucks.**\n","\n","To provide this insight, we will use historical shift sales at each location to build a model. This data has been made available to us in Snowflake.Our model will provide the predicted sales at each location for the upcoming shift.\n","\n"],"id":"3fa8db12-24c0-4fd5-8387-9241b55371c7"},{"cell_type":"markdown","metadata":{"name":"Libraries","collapsed":false},"source":"## Import Packages\n\nJust like the Python packages we are importing, we will import the Snowpark modules that we need.\n\n**Value**: Snowflake modules provide efficient ways to work with data and functions in Snowflake.\n\n","id":"63a66d83-6552-4390-8f4a-abecaffcd076"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Libraries_Code","language":"python","collapsed":false},"outputs":[],"source":"# Import Packages\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport json\nimport sys\nimport cachetools\nimport getpass\n\n# Streamlit\nimport streamlit as st\n\n# Snowpark\nfrom snowflake.snowpark.context import get_active_session\nimport snowflake.snowpark.functions as F\nsession = get_active_session()\n\n# Import Snowflake modules\nimport snowflake.snowpark.types as T\nfrom snowflake.snowpark import Window","id":"78669da6-cf98-4905-8ce7-4d107f566da2"},{"cell_type":"markdown","metadata":{"name":"Part1_PrepareData","collapsed":false},"source":"# Part 1 - Use Snowpark to access and prepare data for modeling","id":"1d0c0e73-1049-4096-b72e-800a41185179"},{"cell_type":"markdown","metadata":{"name":"SnowparkDataFrame"},"source":["## Snowpark DataFrame\n","\n","Let's create a Snowpark DataFrame containing our shift sales data from the **shift_sales_v** view in our Snowflake account using the Snowpark session.table function. A DataFrame is a data structure that contains rows and columns, similar to a SQL table.\n","\n","**Value:** Familiar representation of data for Python users.\n","\n"],"id":"2c0f76db-e5c7-4d4a-92dc-f389f3e41c22"},{"cell_type":"markdown","id":"cd302b2e-10c9-4608-8619-9923a2fee6fc","metadata":{"name":"YOUR_TURN1","collapsed":false},"source":"# YOUR TURN\n\n## Create a Snowflake dataframe called \"snowpark_df\" and then use the show command to see top 10 values\nTable name is your [database name].analytics.shift_sales_v\n\nTIP: Use the following link to find required code https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/index\n"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkDataFrame_Code","language":"python","collapsed":false},"outputs":[],"source":"# insert code to create Snowpark dataframe here","id":"449b12ef-e1a4-4d2e-8925-b62628fbcf4e"},{"cell_type":"markdown","metadata":{"name":"SnowparkDataFrame_View","collapsed":false},"source":"## Preview the Data\n\nWith our Snowpark DataFrame defined, let’s use the .show() function to take a look at the first 10 rows.\n\n**Value:** Instant access to data.\n\n","id":"37430d96-c797-4afb-b6e9-1280d222415f"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkDataFrame_View_Code","language":"python","collapsed":false},"outputs":[],"source":"# Preview the data using the .show() function to look at the first 10 rows.\nsnowpark_df.show()","id":"56bb0880-3518-484e-a77d-cfbf1744da17"},{"cell_type":"markdown","metadata":{"name":"SnowparkDataFrame_Transformations"},"source":["## Select, Filter, Sort\n","\n","Notice the Null values for \"shift_sales\". Let's look at a single location.To do this, we will make another Snowpark DataFrame, location_df, from the above DataFrame and we will:\n","\n","1. Select columns\n","2. Filter to a single location ID\n","3. Sort by date\n","\n","**Value**: Efficient transformation pipelines using Python syntax and chained logic.\n","\n"],"id":"abdd91e2-64ff-4ba0-bbca-18c236b0dbfc"},{"cell_type":"markdown","id":"668c3418-4020-4989-b63d-f4a016dfff7d","metadata":{"name":"YOUR_TURN2","collapsed":false},"source":"# YOUR TURN\n\n## Create a Snowflake dataframe called location_df that only includes: date, shift, shift_sales, location_id, and city\n## Then filter the dataframe for location_id = 1135\n## Then order the dataframe by date and shift\n## Visualize the first 10 records\n\n\nTIP: Use the following link to find required code https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/index\n"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkDataFrame_Transformations_Code","language":"python","collapsed":false},"outputs":[],"source":"# Select\nlocation_df = ????\n\n# Filter\nlocation_df = ????\n\n# Sort\nlocation_df = ????\n\n# Display\nlocation_df.show(n=??)","id":"2ee49ad3-0979-4363-8a43-0dc33e4c234c"},{"cell_type":"markdown","metadata":{"name":"Snowpark_How","collapsed":false},"source":"We can see that shift sales are populated 8 days prior to the latest date in the data. The **missing values** represent future dates that do not have shift sales yet.\n\n## Snowpark works in two main ways:\n\n1. Snowpark code translated and executed as SQL on Snowflake\n2. Python functions deployed in a secure sandbox in Snowflake","id":"cb6d2ad4-4c91-43a5-83ac-05c2800ffdce"},{"cell_type":"markdown","metadata":{"name":"Snowpark_ExplainQuery","collapsed":false},"source":"## Explain the Query\n\nLet's look at what was executed in Snowflake to create our location_df DataFrame.\n\nThe translated SQL query can be seen in the Snowsight interface under _Activity_ in the _Query History_ or directly in our notebook by using the explain() function. \n\n**Value:** Transparent execution and compute usage.","id":"08f1b00b-3560-49a4-a7ea-523fd16687c8"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_Explain","language":"python","collapsed":false},"outputs":[],"source":["location_df.explain()"],"id":"8db7ed9c-b597-4737-8b09-43540b9d17b4"},{"cell_type":"markdown","metadata":{"name":"CompareSFDataFramevsPandasDataFrame"},"source":["## Compare DataFrame Size\n","\n","Let's bring a sample of our Snowflake dataset to our Python environment in a pandas DataFrame using the to_pandas() function. We will compare how much memory is used for the pandas DataFrame compared to the Snowpark DataFrame. As we will see, no memory is used for the Snowpark DataFrame in our Python environment. All data in the Snowpark DataFrame remains on Snowflake.\n","**Value:** No copies or movement of data when working with Snowpark DataFrames.\n","\n"],"id":"0ce2b480-b880-4fdd-aa3c-3c94d7d0c0cf"},{"cell_type":"code","execution_count":null,"metadata":{"name":"CompareSFDataFramevsPandasDataFrame_Code","language":"python","collapsed":false},"outputs":[],"source":["# Bring 10,000 rows from Snowflake to pandas\n","pandas_df = snowpark_df.limit(10000).to_pandas()\n","\n","# Get Snowpark DataFrame size\n","snowpark_size = sys.getsizeof(snowpark_df) / (1024*1024)\n","print(f\"Snowpark DataFrame Size (snowpark_df): {snowpark_size:.2f} MB\")\n","\n","# Get pandas DataFrame size\n","pandas_size = sys.getsizeof(pandas_df) / (1024*1024)\n","print(f\"Pandas DataFrame Size (pandas_df): {pandas_size:.2f} MB\")"],"id":"3b78bc1f-4154-4288-b2b2-85c496a7829d"},{"cell_type":"markdown","metadata":{"name":"Snowpark_DataExploration","collapsed":false},"source":"## Data Exploration\n\nHere, we will use Snowpark to explore our data. A common pattern for exploration is to use Snowpark to manipulate our data and then bring an aggregate table to our Python environment for visualization.\n\n**Value:** - Native Snowflake performance and scale for aggregating large datasets. - Easy transfer of aggregate data to the client-side environment for visualization.\nAs we explore our data, we will highlight what is being done in Snowflake and what we are transferring to our client-side environment (Python notebook environment) for visualization.\n\n","id":"eb800752-b83e-4340-9e1d-d3da732c7fb9"},{"cell_type":"markdown","metadata":{"name":"SnowparkDF_Count","collapsed":false},"source":"## How many rows are in our data?\n\nThis will give us an idea of how we might need to approach working with this data. Do we have enough data to build a meaningful model? What compute might be required? Will we need to sample the data?\n\n**What's happening where?:** Rows counted in Snowflake. No data transfer.\n\n","id":"276e6283-133c-4531-bf10-b1314521b1e2"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkDF_Count_Code","language":"python","collapsed":false},"outputs":[],"source":["#Use the .count() function\n","snowpark_df.count()"],"id":"d57909f6-d03b-4f12-9ca0-e831ed44107f"},{"cell_type":"code","id":"5dc81ecc-d844-45c7-aa54-fb8b931b0e92","metadata":{"language":"python","name":"SnowparkDF_Count_SiS","collapsed":false},"outputs":[],"source":"df = session.table('HOL.SCHEMA0.SHIFT_SALES_V').group_by(F.col('CITY')).agg(F.count('LOCATION_ID').alias('total_locations'))\nst.bar_chart(data=df,x='CITY',y='TOTAL_LOCATIONS')","execution_count":null},{"cell_type":"markdown","metadata":{"name":"Snowpark_Describe","collapsed":false},"source":"## Let's calculate some descriptive statistics.\n\nWe use the Snowpark describe() function to calculate summary statistics and then bring the aggregate results into a pandas DataFrame to visualize in a formatted table.\n\n**What's happening where?:** Summary statistics calculated in Snowflake. Transfer aggregate summary statistics for client-side visualization.\n\n","id":"271fb4e5-0907-4d0d-bb63-85a79b702f44"},{"cell_type":"markdown","id":"f5da0300-899e-41d0-b410-b2de70be4a0c","metadata":{"name":"YOUR_TURN3","collapsed":false},"source":"# YOUR TURN\n\n## Use the built-in Snowpark function to generate descriptive statistics for the Snowpark dataframe called \"snowpark_df\"\n\n\nTIP: Use the following link to find required code https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/index\n"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_Describe_Code","language":"python","collapsed":false},"outputs":[],"source":"# Use the Snowpark DataFrame .describe function. You need to need to visualize from a pandas DataFrame\n\n# insert code to use Snowpark .describe function here and visualize using pandas format","id":"8a73b888-fb61-46c2-8571-98b7e9932b43"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_PreForModeling_Numeric","language":"python","collapsed":false},"outputs":[],"source":["# What are the numeric columns?\n","# Define Snowflake numeric types\n","numeric_types = [T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType, T.LongType]\n","\n","# Get numeric columns\n","numeric_columns = [col.name for col in snowpark_df.schema.fields if type(col.datatype) in numeric_types]\n","numeric_columns"],"id":"10b3b4af-2182-411f-bbc3-c4f356c701ed"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_PreForModeling_Categorical","language":"python","collapsed":false},"outputs":[],"source":["# What are the categorical columns?\n","# Define Snowflake categorical types\n","categorical_types = [T.StringType]\n","\n","# Get categorical columns\n","categorical_columns = [col.name for col in snowpark_df.schema.fields if type(col.datatype) in categorical_types]\n","categorical_columns"],"id":"003897a5-2f35-468d-bf7e-8777a37d19b5"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_Explore_SiS","language":"python","collapsed":false},"outputs":[],"source":"# What are the average shift sales (USD) by city?\n# Group by city and average shift sales\ndf = snowpark_df.group_by(\"city\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n\n# Pull to pandas and plot\nst.bar_chart(data=df,x='CITY',y='AVG_SHIFT_SALES')","id":"8665c3bf-1b18-402f-9601-76e7bd2968ed"},{"cell_type":"markdown","metadata":{"name":"Snowpark_FeatureEngineering","collapsed":false},"source":"## Feature Engineering\n\nNow let's keep revelant columns and transform columns to create features needed for our prediction model.To make some of our features more useful, we will normalize them using standard preprocessing techniques, such as One-Hot Encoding and MinMaxScaling. With SnowparkML, you can use a standard sklearn-style API to execute fully distributed feature engineering preprocessing tasks on Snowflake compute, with zero data movement. Let's fit a scaler and encoder to our data, then use it to transform the data, producing new feature columns.\n\n\n**Value:** The Snowpark syntax makes pipelines easy to implement and understand. The syntax also allows for easy migration of Spark pipelines to Snowflake.\n\n\n**All transformations for feature engineering in this notebook will be executed on Snowflake compute.**\n\nNotice what we haven't had to do? No tuning, maintenance, or operational overhead. We just need a role, warehouse, and access to the data.\n\n**Value**: Near-zero maintenance. Focus on the work that brings value.\n\n","id":"d2bd04e5-acfa-4f3a-9fab-fa73c285685b"},{"cell_type":"markdown","metadata":{"name":"Snowpark_FeatureEngineering_RollingAverage"},"source":["## Create a Rolling Average Feature\n","\n","We will use a Snowflake window function to get a **rolling shift average by location** over time. Window functions allow us to aggregate on a \"moving\" group of rows.\n","\n","#### **Step 1. Create a Window**\n","\n","Our window will partition the data by location and shift. It will order rows by date. It will include all rows prior to the current date of the observation it is aggregating for.\n","\n"],"id":"eebb0bf0-0026-4dce-b913-a3a8c1c94216"},{"cell_type":"markdown","id":"7aa0706b-f336-455b-8e28-c9da1238e24e","metadata":{"name":"YOUR_TURN4","collapsed":false},"source":"# YOUR TURN\n\n## Use a rolling average in two steps. First, create a window_by_location_all_days by location_id and shift which is ordered by date. Then create an aggregate average location shift sales.\n\nTIP: Use the following link to find required code https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/index\n"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_FeatureEngineering_RollingAverage_CreateWindowCode","language":"python","collapsed":false},"outputs":[],"source":"window_by_location_all_days = (\n    Window.partition_by(???, ???)\n    .order_by(???)\n    .rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW - 1)\n)","id":"08a80979-6c77-48b9-9ddc-8fb1e5f013bd"},{"cell_type":"markdown","metadata":{"name":"Snowpark_FeatureEngineering_AggregateWindows"},"source":["#### **Step 2. Aggregate across the Window**\n","\n"],"id":"2a392ecc-89d5-4c9e-8e35-2ff1047e2f44"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_FeatureEngineering_AggregateWindowsCode","language":"python","collapsed":false},"outputs":[],"source":"snowpark_df = snowpark_df.with_column(\n    ???, \n    F.avg(???).over(window_by_location_all_days)\n)","id":"06722409-6f5a-478d-ace6-47d9eef38b4b"},{"cell_type":"markdown","metadata":{"name":"Snowpark_ImputeMIssingValues"},"source":["## Impute Missing Values\n","\n","The rolling average feature we just created is missing if there are no prior shift sales at that location. We will replace those missing values with 0.\n","\n"],"id":"dc1cefbe-a91b-45d8-a22a-aa52b6ebe838"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_ImputeMIssingValues_Code","language":"python","collapsed":false},"outputs":[],"source":["snowpark_df = snowpark_df.fillna(value=0, subset=[\"avg_location_shift_sales\"])"],"id":"523adbc7-205c-463f-acd2-d73c8cb1889c"},{"cell_type":"markdown","metadata":{"name":"SnowparkMLModelingAPI","collapsed":false},"source":"## Leverage Snowpark ML Modeling API to create features\n\nSnowpark ML provides APIs to support each stage of an end-to-end machine learning development and deployment process and includes two key components: [Snowpark ML Modeling](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling) and [Snowpark ML Ops](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry).\n\n[Snowpark ML Modeling](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling) supports data preprocessing, feature engineering, and model training in Snowflake using popular machine learning frameworks, such as scikit-learn, xgboost, and lightgbm. This API also includes a preprocessing module that can use compute resources provided by a Snowpark-optimized warehouse to provide scalable data transformations.\n\nSnowpark ML Operations (MLOps), featuring the [Snowpark ML Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry), complements the Snowpark ML Development API. The model registry allows secure deployment and management of models in Snowflake, and supports models trained both inside and outside of Snowflake.","id":"5b8d2565-0726-4f1e-a842-edff14425bac"},{"cell_type":"markdown","id":"b36b63e5-5bb8-4028-83a7-2222d26fd1be","metadata":{"name":"YOUR_TURN5","collapsed":false},"source":"# YOUR TURN\n\n## Scale up your assigned Snowflake compute warehouse.  Enter your warehouse name."},{"cell_type":"code","execution_count":null,"metadata":{"name":"AlterWarehouseSizeUp","language":"python","collapsed":false},"outputs":[],"source":"# insert code to scale up Snowflake compute warehouse to size Large","id":"a0ade1af-883d-4843-bf9d-6fa61a449fa0"},{"cell_type":"markdown","id":"d2f9766d-bf14-4595-b62a-2944d1be01a2","metadata":{"name":"YOUR_TURN6","collapsed":false},"source":"# YOUR TURN\n\n## Use Snowpark ML API to normalize the data using the fit_scaler function on numeric columns\nTIP: use help(snowmlpp.MinMaxScaler)\nSnowpark ML scaler (MinMaxScaler) is used to shrink data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution. For the Tasty_Bytes data, use MinMaxScaler to normalize \"CITY_POPULATION\" into \"CITY_POPULATION_NORM\" with values between 0 and 1.\n\n## Use Snowpark ML API to perform one-hot encoding function on categorical columns\nTIP: use help(snowmlpp.OrdinalEncoder)\nSnowpark ML ordinal encoding (OE) is used to improve model performance by providing more information to the model about categorical variables. # It can help to avoid the problem of ordinality, which can occur when a categorical variable has a natural ordering (e.g. “small”, “medium”, “large”).# For the Tasty_Bytes data, use OE to change \"SHIFT\" which is currently AM or PM into and integer representation of \"SHIFT_OHE\" is 1.0 or 0.0. \n"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkMLModelingAPI_Code","language":"python","collapsed":false},"outputs":[],"source":"# Import Snowpark ML: Machine Learning Toolkit for Snowflake\nimport snowflake.ml.modeling.preprocessing as snowmlpp\n\n# Snowpark ML scaler (MinMaxScaler) is used to shrink data within the given range, usually of 0 to 1. \n\ndef fit_scaler(session, df):\n    mm_target_columns = [???]\n    mm_target_cols_out = [???]\n    snowml_mms = snowmlpp.MinMaxScaler(input_cols=mm_target_columns, output_cols=mm_target_cols_out)\n    snowml_mms.fit(df)\n    return snowml_mms\n\n# Snowpark ML ordinal encoding (OE) is used to improve model performance by providing more information to the model about categorical variables. \n\ndef fit_oe(session, df):\n    oe_target_cols = [???]\n    oe_output_cols = [???]\n    snowml_oe = snowmlpp.OrdinalEncoder(input_cols=oe_target_cols, output_cols=oe_output_cols)\n    snowml_oe.fit(df)\n    return snowml_oe","id":"f2e664e2-e37a-4599-bef3-bd47d870732a"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkMLModelingAPI_PreprocessingFunctionsCode","language":"python","collapsed":false},"outputs":[],"source":"# Run Snowpark ML preprocessing functions against our feature data\n\n# For the Tasty_Bytes data, use MinMaxScaler to normalize \"CITY_POPULATION\" into \"CITY_POPULATION_NORM\" with values between 0 and 1.\nsnowml_mms = fit_scaler(session, snowpark_df)\nnormed_df = snowml_mms.transform(snowpark_df)\n\n# For the Tasty_Bytes data, use OneHotEncoder to change \"SHIFT\" which is currently AM or PM into and integer representation of \"SHIFT_OHE_AM\" is 1 or 0 and \"SHIFT_OHE_PM\" is 1 or 0. \nsnowml_oe = fit_oe(session, normed_df)\noe_df = snowml_oe.transform(normed_df)\noe_df.show()","id":"7bef3b90-ee88-4202-a0c5-07d04d9fe691"},{"cell_type":"markdown","metadata":{"name":"Snowpark_CreateHoldoutData"},"source":["## Filter to Historical Data\n","\n","Our data includes placeholders for future data with missing shift sales. The future data represents the next 7 days of shifts for all locations. The historical data has shift sales for all locations where a food truck parked during a shift. We will only use historical data when training our model and will filter out the dates where the shift_sales column is missing.\n","\n"],"id":"2ae7ff76-59bc-4aea-adad-90a8acbaf60e"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_CreateHoldoutData_Code","language":"python","collapsed":false},"outputs":[],"source":["# Data Science best practice: Always perform data quality on your training set e.g. remove nulls or invalid cells as they are the biggest problem in a training set as they output high false positives\n","\n","# Specifically for Tasty_Bytes data, dates where \"shift_sales\" are null values reflect future dates where sales need to be predicted.\n","# Filter out these future dates so these records will not be used in model training. \n","historical_df = oe_df.filter(F.col(\"shift_sales\").is_not_null())"],"id":"411ec9ff-3751-4b71-b5ac-4be29cef27d3"},{"cell_type":"markdown","metadata":{"name":"Snowpark_PersistTransformations"},"source":["## Persist Transformations\n","\n","If we want to save the changes we can either save it as a table, meaning the SQL generated by the DataFrame is executed and the result is stored in a table or as a view where the DataFrame SQL will be the definition of the view.\n","**save_as_table** saves the result in a table, if **mode='overwrite'** then it will also replace the data that is in it.\n","\n"],"id":"b77f1264-42f7-4f7f-993a-07d143e15e72"},{"cell_type":"markdown","id":"f7cb2814-360e-48b0-87a3-312d4c50724c","metadata":{"name":"YOUR_TURN7","collapsed":false},"source":"# YOUR TURN\n\n## Specify a table name to save your Snowpark DataFrame."},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_PersistTransformations_Code","language":"python","collapsed":false},"outputs":[],"source":"# enter your code to save historical.df as INPUT_DATA in database HOL and your assigned schema","id":"e19f47fa-281c-49c0-8f45-d9630662dafb"},{"cell_type":"markdown","metadata":{"name":"Part2","collapsed":false},"source":"# Part 2 - Use Snowflake Cortex ML-Based Function for Time-Series Forecasting","id":"733f962b-27b3-42da-b27e-47c7ccc83e6e"},{"cell_type":"markdown","metadata":{"name":"SnowflakeCortex_MLForecasting","collapsed":false},"source":"## Snowflake Cortex ML Functions\n\nTime-Series Forecasting is part of Snowflake Cortex, Snowflake’s intelligent, fully-managed AI and ML service. This feature is part of the Snowflake Cortex ML-based function suite. Forecasting employs a machine learning algorithm to predict future data by using historical time series data.\n\nTime series forecasting produces univariate predictions of future data based on historical input data. A common use case is to forecast sales based on seasonality and other factors.The historical data must include:\n\n- A timestamp column.\n- A target value column representing some quantity of interest at each timestamp.\n\nThe historical data can also include additional columns that might have influenced the target value ([exogenous variables](https://en.wikipedia.org/wiki/Exogenous_and_endogenous_variables)). These can be numbers or text. The nature (categorical or continuous) of each such column is automatically detected.\n\nThis historical data is used to train a machine learning model that produces a forecast of the value column at future timestamps. The model is a schema-level object and can be used for multiple forecasts after it is trained.\n\nForecasting works with either single-series or multi-series data. Multi-series data represents multiple independent threads of events. For example, if you have sales data for multiple stores, each store’s sales can be forecast separately by a single model based on the store identifier.\n\nTo produce forecasts of time series data, use the Snowflake built-in class [FORECAST](https://docs.snowflake.com/en/sql-reference/classes/forecast), and follow these steps:\n\n1. [Create a forecast model object](https://docs.snowflake.com/en/sql-reference/classes/forecast.html#label-class-forecast-create) passing in a reference to the training data.This object will fit (train) a model to the training data that you provide. The model is a schema-level object.\n2. Using this forecast model object, call [CREATE SNOWFLAKE.ML.FORECAST](https://docs.snowflake.com/en/sql-reference/classes/forecast.html#label-class-forecast-create) to produce a forecast, passing in information about the future period (that is, the number of time steps and values for any non-timestamp features).The method uses the model to produce a forecast.\n\n#### About the Forecasting Algorithm\n\nThe forecasting algorithm is powered by a [gradient boosting machine](https://en.wikipedia.org/wiki/Gradient_boosting) (GBM). Like an [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) model, it uses a differencing transformation to model data with a non-stationary trend and uses auto-regressive lags of the historical target data as model features.\n\nAdditionally, the algorithm uses rolling averages of historical target data to help predict trends and automatically produces cyclic calendar features (such as day of week and week of year) from timestamp data.\n\nYou can fit models with only historical target and timestamp data, or you may include exogenous data (features) that might have influenced the target value. Exogenous variables can be numerical or categorical and may be NULL (rows containing NULLs for exogenous variables are not dropped).\n\nThe algorithm does not rely on one-hot encoding when training on categorical features, so you can use categorical data with many dimensions (high cardinality).\n\nFor more details about Snowflake's ML-Powered Forecasting Algorithm and how to use, please see [https://docs.snowflake.com/en/user-guide/ml-powered-forecasting#about-the-forecasting-algorithm](https://docs.snowflake.com/en/user-guide/ml-powered-forecasting#about-the-forecasting-algorithm)","id":"7a411b60-e702-4acf-8f8e-f3d54b3ccd77"},{"cell_type":"markdown","id":"3d774dea-1f8a-4571-970c-9469e8358d74","metadata":{"name":"YOUR_TURN8","collapsed":false},"source":"# YOUR TURN\n\n## Write SQL code to view the the SALES_FORECAST_INPUT data."},{"cell_type":"code","execution_count":null,"metadata":{"name":"SQL_ViewTable_Code","language":"sql","collapsed":false},"outputs":[],"source":"SELECT * FROM ??? LIMIT 10;","id":"d40ed87b-8fd4-4802-9d56-ac4364cff5bd"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SQL_CreateView_Code","language":"sql","collapsed":false},"outputs":[],"source":"CREATE OR REPLACE VIEW lobster_sales AS \n(SELECT timestamp, total_sold FROM HOL.SCHEMA0.SALES_FORECAST_INPUT WHERE menu_item_name LIKE 'Lobster Mac & Cheese');","id":"f85bc4f9-fc81-4009-ab7f-2811e791f30c"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SQL_CreateCortexMLForecast_Code","language":"sql","collapsed":false},"outputs":[],"source":"-- Create Cortex ML forecast called lobstermac_forecast\nCREATE OR REPLACE snowflake.ml.forecast lobstermac_forecast(INPUT_DATA => SYSTEM$REFERENCE('VIEW', 'lobster_sales'),TIMESTAMP_COLNAME => 'TIMESTAMP',TARGET_COLNAME => 'TOTAL_SOLD');","id":"f60bbb4b-82bd-4ee2-b5e7-d598056eac54"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SQL_CreateCortexMLForecastShowModels_Code","language":"sql","collapsed":false},"outputs":[],"source":"-- Show models to confirm training has completed: \nSHOW snowflake.ml.forecast;","id":"ce2764bf-1590-4264-ab42-3e528e21953c"},{"cell_type":"markdown","metadata":{"name":"Part3","collapsed":false},"source":"# Part 3 - Use Snowpark to train a model","id":"cd41e298-f6d3-4b8c-9206-ebb214565053"},{"cell_type":"markdown","metadata":{"name":"Snowpark_ModelPrep"},"source":["## Drop Columns\n","\n","Let's return to the original prepared table, with all cities listed, and drop columns that will not be used in the model.\n","\n"],"id":"80a40aff-0039-4bda-ab7b-ff98ff42ca1e"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_ModelPrep_Code","language":"python","collapsed":false},"outputs":[],"source":["prepared_df = historical_df.drop(\"location_id\", \"city_population\", \"shift\", \"city\", \"date\")\n","prepared_df.show()"],"id":"2a93dfbd-ad07-46c8-b917-defd5bdf61fa"},{"cell_type":"markdown","metadata":{"name":"Snowpark_XGBoostRegressorModel"},"source":["## Build a simple XGBoost Regression Model on Snowflake\n","\n","We will now use our training data to train a linear regression model on Snowflake.Recall from above, the two main ways that Snowpark works:\n","\n","1. Snowpark code translated and executed as SQL on Snowflake\n","2. Python functions deployed in a secure sandbox in Snowflake\n","\n","We will be leveraging the deployment of Python functions into Snowflake for training and model deployment.\n","\n"],"id":"db7f8fa5-bce9-44aa-b25f-845865d59f79"},{"cell_type":"markdown","id":"788a601a-66f6-49a9-8195-ab1ffa83d408","metadata":{"name":"YOUR_TURN9","collapsed":false},"source":"# YOUR TURN\n\n## Define relevant LABEL_COLUMNS and OUTPUT_COLUMNS for your XGBoost Regressor model\n\nTIP: Look up code in https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling#constructing-a-model"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_XGBoostRegressorModel_Columns","language":"python","collapsed":false},"outputs":[],"source":["# Retrieve column names needed in the next code block to populate feature_column_names\n","prepared_df.columns"],"id":"e15ac942-7134-4b08-9f16-dcb1f6be32a5"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_XGBoostRegressorModelSetup_Code","language":"python","collapsed":false},"outputs":[],"source":"# Let's define relevant features needed for the prediction model.\nLABEL_COLUMNS = [???]\nOUTPUT_COLUMNS = [???]\nFEATURE_COLUMN_NAMES = [\"SHIFT_OE\", \"CITY_POPULATION_NORM\", \"MONTH\", \"DAY_OF_WEEK\",\"LATITUDE\",\"LONGITUDE\",\"AVG_LOCATION_SHIFT_SALES\"]\n\ninput_df = prepared_df.select(*LABEL_COLUMNS, *FEATURE_COLUMN_NAMES)\ninput_df.show()","id":"caa7e956-b69f-4437-bb15-d95c99b89944"},{"cell_type":"markdown","metadata":{"name":"SnowparkML"},"source":["SnowparkML also includes metric calculations such as correlations, and more. We will use the SnowparkML correlation method on our input dataframe to identify any linearly correlated features to the output. We'll also use matplotlib to plot the resulting matrix. Notice that all of the correlation calculations are pushed down to Snowflake!\n","\n"],"id":"b18c401c-6e4c-4b45-a684-f11f8c0bb323"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkML_Correlation_Code","language":"python","collapsed":false},"outputs":[],"source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom snowflake.ml.modeling.metrics.correlation import correlation\ncorr_df = correlation(df=input_df)\n\nfig, ax = plt.subplots()\nsns.heatmap(corr_df.corr(), ax=ax)\nst.write(fig)","id":"31a22f10-17ee-4129-a3b7-3b6f5fe8e8fe"},{"cell_type":"markdown","metadata":{"name":"SnowparkML_SplitData4Modeling"},"source":["What's great about this, is that we are using a lot of Snowpark components under the hood- the dataframe API, SQL, Python stored procedures and more. But with the new SnowparkML API, data scientists can take advantage of all that Snowpark affords them, while using common, familiar APIs that match how they do their work today.\n","\n","Now that we have our feature data, let's actually fit an XGBoost model to our features to attempt to predict future sales. We'll fit several different models with different hyperparameters, and then show how we can use the Snowpark Model Registry to select our best-fit model.\n","\n"],"id":"8139efe5-f965-46cb-8bd7-6dfa0798ae9d"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkML_SplitData4Modeling_Code","language":"python"},"outputs":[],"source":["# Split the data into train and test sets\n","train_df, test_df = input_df.random_split(weights=[0.9, 0.1], seed=98)"],"id":"c82c4134-289e-4482-b24e-bfc57ef5071c"},{"cell_type":"markdown","metadata":{"name":"SnowparkMLAPI_Modeling","collapsed":false},"source":"## What's happening when you leverage Snowpark ML Modeling API?\n\nLet's run our training job using the SnowparkML Modeling API- this will push down our model training to run on Snowflake, and you'll notice that the type of the model object returend is a SnowparkML XGBClassifier- this has some benefits, but also is fully compatible with the standard sklearn/xgboost model objects.\n\n- The model.fit() function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation. Be sure to use a Snowpark Optimized Warehouse if you need more memory. We are just using an XS Standard Virtual Warehouse here, which we created at the beginning of this quickstart.\n- The model.predict() function actually creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data. You can check the query history once you execute the following cell to check.","id":"edac01ae-6e20-41ff-8377-e9fed0014eb7"},{"cell_type":"markdown","id":"7e8c194d-451e-4017-9fbb-289967f67cdd","metadata":{"name":"YOUR_TURN10","collapsed":false},"source":"# YOUR TURN\n\n## Import and define all the relevant details to train an XGBoost Regressor model\n\nTIP: Look up code in https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling#constructing-a-model"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkMLAPI_Modeling_Code","language":"python"},"outputs":[],"source":"from snowflake.ml.modeling.??? import ???\n# Define the XGBRegressor\nregressor = ???(\n    label_cols = ???,\n    input_cols = ???,\n    output_cols = ???\n)\n\n# Train\nregressor.fit(train_df)\n\n# Predict\nresult = regressor.predict(test_df)","id":"c3179b2b-be59-41e2-961c-449efb2def2a"},{"cell_type":"code","execution_count":null,"metadata":{"name":"Snowpark_MLAPI_ModelPredictOutput_Code","language":"python","collapsed":false},"outputs":[],"source":"# Just to illustrate, we can also pass in a Pandas DataFrame to Snowpark ML's model.predict()\nregressor.predict(test_df.to_pandas())","id":"e809f760-f997-44d8-9c84-ee63a29c71d3"},{"cell_type":"markdown","id":"bd4cc96f-d7cb-4022-9207-b98bc6cbc0f4","metadata":{"name":"YOUR_TURN11","collapsed":false},"source":"# YOUR TURN\n\n## Complete and run the mse (mean square error) for your XGBoost Regressor model results\n\nTIP: Look up code in https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling#constructing-a-model"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkMLAPI_ModelAccuracy_Code","language":"python"},"outputs":[],"source":"# Use Snowpark ML metrics to calculate\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error, mean_squared_error\n\n# Predict\nresults = regressor.predict(test_df)\n\n# Calculate MAPE\nmape = mean_absolute_percentage_error(df=results, y_true_col_names=LABEL_COLUMNS, y_pred_col_names=OUTPUT_COLUMNS)\n\n# Calculate MSE\nmse = ???\n\nresults.select([*LABEL_COLUMNS, *OUTPUT_COLUMNS]).show()\nprint(f'''Mean absolute percentage error: {mape}''')\nprint(f'''Mean squared error: {mse}''')","id":"85674393-aeed-406c-a802-db292b597454"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkMLAPI_ModelAccuracy_SiS","language":"python","collapsed":false},"outputs":[],"source":"# Plot actual vs predicted \ng = sns.relplot(data=results[\"SHIFT_SALES\", \"PRED_SHIFT_SALES\"].to_pandas().astype(\"float64\"), x=\"SHIFT_SALES\", y=\"PRED_SHIFT_SALES\", kind=\"scatter\")\ng.ax.axline((0,0), slope=1, color=\"r\")\n\n# Display the plot in Streamlit\nst.pyplot(g.fig)","id":"957b744c-12d9-4e60-9542-5a6fffd26379"},{"cell_type":"markdown","metadata":{"name":"SnowparkMLAPI_GridSearch"},"source":["## Snowpark ML's GridSearchCV()\n","\n","Now, let's use Snowpark ML's GridSearchCV() function to find optimal model parameters.\n","\n"],"id":"bd32172b-7aab-48bc-9852-8b2ff136ee46"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkMLAPI_GridSearch_Code","language":"python","collapsed":false},"outputs":[],"source":["from snowflake.ml.modeling.model_selection import GridSearchCV\n","\n","grid_search = GridSearchCV(\n","estimator=XGBRegressor(),\n","param_grid={\n","\"n_estimators\":[25, 50],\n","\"learning_rate\":[0.4, 0.5],\n","},\n","n_jobs = -1,\n","scoring=\"neg_mean_absolute_percentage_error\",\n","input_cols=FEATURE_COLUMN_NAMES,\n","label_cols=LABEL_COLUMNS,\n","output_cols=OUTPUT_COLUMNS\n",")\n","\n","\n","# Train\n","grid_search.fit(train_df)"],"id":"22005572-4302-43d8-982e-ef903a879024"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkMLAPI_GridSearch_ModelImprovement_Code","language":"python","collapsed":false},"outputs":[],"source":"# Let's analyze the grid_search results\ngs_results = grid_search.to_sklearn().cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"]*-1\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"mape\":mape_val})\n\ng2 = sns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n\n# Display the plot in Streamlit\nst.pyplot(g2.fig)","id":"0ad4f16a-8baf-470e-b0fc-d7d13e128990"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowparkMLAPI_OptimalModel_Code","language":"python","collapsed":false},"outputs":[],"source":["# Let's save our optimal model and its metadata:\n","optimal_model = grid_search.to_sklearn().best_estimator_\n","optimal_n_estimators = grid_search.to_sklearn().best_estimator_.n_estimators\n","optimal_learning_rate = grid_search.to_sklearn().best_estimator_.learning_rate\n","\n","\n","optimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n","                        (gs_results_df['learning_rate']==optimal_learning_rate),'mape'].values[0]"],"id":"bc418ad8-c865-401e-b5a3-60647ae00954"},{"cell_type":"markdown","metadata":{"name":"ScaleDownWarehouse"},"source":["## **Scale down your assigned Snowflake compute warehouse.**\n","\n"],"id":"8ba3f0db-5bd5-4673-8f31-97887b0a11fe"},{"cell_type":"markdown","id":"fc1e198a-37d6-4d2e-ba28-c7b022ea16d5","metadata":{"name":"YOUR_TURN12","collapsed":false},"source":"# YOUR TURN\n\n## Scale down your assigned Snowflake compute warehouse to XSmall."},{"cell_type":"code","execution_count":null,"metadata":{"name":"ScaleDownWarehouse_Code","language":"python","collapsed":false},"outputs":[],"source":"# Decrease size of Snowflake compute warehouse to XSMALL\n# insert your code to scale down your assigned Snowflake compute warehouse to XSmall","id":"7afd018d-eb9f-408d-8b34-c2721cf4bbcb"},{"cell_type":"markdown","metadata":{"name":"Part4","collapsed":false},"source":"# Part 4 - Use Snowpark for MLOps","id":"696b4aad-2ab7-4b60-ac9d-a4d3234a2895"},{"cell_type":"markdown","metadata":{"name":"SnowflakeModelRegistry","collapsed":false},"source":"## Now let's use Snowflake's ML Model Registry\n\nModel Registry was created to support model management operations including model registration, versioning, metadata and audit trails. Integrated deployment infrastructure for batch inference is a critical ease-of-use feature. Users can deploy ML models for batch inference from the registry directly into a Snowflake Warehouse as a vectorized UDF, or as a service to a customer-specified Compute Pool in Snowpark Container Services.\n\nSnowflake's Model Registry supports SciKitLearn, XGBoost, Pytorch, Tensorflow and MLFlow (via the pyfunc interface) models.\n\nModel Registry allows easy deployment of pre-trained open-source models from providers such as HuggingFace. See this blog for more details: [https://medium.com/snowflake/whats-in-a-name-model-naming-versioning-in-snowpark-model-registry-b5f7105fd6f6](https://medium.com/snowflake/whats-in-a-name-model-naming-versioning-in-snowpark-model-registry-b5f7105fd6f6) or the Model Registry documentation: https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry#deleting-models","id":"5dbfe1d1-fbf4-429d-9a8f-0b9e2e5dccf3"},{"cell_type":"markdown","id":"fd409b74-7db4-4508-bcb6-aa4985052b2c","metadata":{"name":"YOUR_TURN13","collapsed":false},"source":"# YOUR TURN\n\n## Define your model to be registered into the Snowflake Model Registry. Suggest using \"SHIFT_SALES_PREDICTION\" and version \"VO\". "},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowflakeModelRegistry_CreateRegisterModel_OriginalModel_Code","language":"python","collapsed":false},"outputs":[],"source":"# Create the Model Registry and register your initial model\nfrom snowflake.ml.registry import Registry\n\nnative_registry = Registry(session, database_name=\"HOL\", schema_name=\"SCHEMA0\")\n# Get sample input data to pass into the registry logging function\nX = train_df.select(FEATURE_COLUMN_NAMES).limit(100)\n\n# Define model name\nmodel_name = ???\n\n# Let's first log the very first model we trained\nmodel_ver = native_registry.log_model(\n    model_name=model_name,\n    version_name=???,\n    model=regressor,\n    sample_input_data=X, # to provide the feature schema\n)\n# Add a description\nmodel_ver.comment = \"This is the initial model of the Shift Sales Price Prediction model.\"","id":"703db0dc-670a-4d44-8459-4fd0f77d3e53"},{"cell_type":"code","id":"b2182c38-1f3a-4536-92f6-7a0006f73122","metadata":{"language":"python","name":"SnowparkModelRegisty_DeleteModels_IfNeeded_Code","collapsed":false},"outputs":[],"source":"# Here is the code to delete model(s) named \"SHIFT_SALES_PREDICTION\" within the Snowflake Model Registry\n# native_registry.delete_model(\"SHIFT_SALES_PREDICTION\")","execution_count":null},{"cell_type":"markdown","id":"2e0829c0-85a1-44ba-b57d-70980f4203cd","metadata":{"name":"YOUR_TURN14","collapsed":false},"source":"# YOUR TURN\n\n## Register the optimal model from the GridSearchCV output. Suggest using version \"V2\"."},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowflakeModelRegistry_RegisterOptimalModel_Code","language":"python","collapsed":false},"outputs":[],"source":"# Now, let's log the optimal model from GridSearchCV\nmodel_ver2 = native_registry.log_model(\n    model_name=model_name,\n    version_name=???,\n    model=???,\n    sample_input_data=X, # to provide the feature schema\n)\n\n# Add evaluation metric\nmodel_ver2.set_metric(metric_name=???, value=optimal_mape)\n\n# Add a description\nmodel_ver2.comment = \"???\"","id":"d6e9ef6e-4b91-4da3-ba51-af552d07dc16"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowflakeModelRegistry_ShowVersions_Code","language":"python","collapsed":false},"outputs":[],"source":["# Let's confirm model(s) that were added\n","native_registry.get_model(model_name).show_versions()"],"id":"a4875ffe-5d80-4bde-b3fa-2b295f3abff9"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowflakeModelRegistry_ShowDefaultModel_Code","language":"python","collapsed":false},"outputs":[],"source":["# We can see what the default model is when we have multiple versions with the same model name:\n","native_registry.get_model(model_name).default.version_name"],"id":"d511dc1c-9cb1-4572-b90d-6940eebfdf49"},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowflakeModelRegistry_ModelInference_Code","language":"python","collapsed":false},"outputs":[],"source":["# Now we can use the default version model to perform inference.\n","model_ver = native_registry.get_model(model_name).version('V0')\n","result_sdf = model_ver.run(test_df, function_name=\"predict\")\n","result_sdf.show()"],"id":"462fcbcc-479b-47b8-a99a-6e7cc518276d"},{"cell_type":"markdown","id":"afe05ab0-2f4d-42b5-93dd-f4c0f542ec50","metadata":{"name":"YOUR_TURN15","collapsed":false},"source":"# YOUR TURN\n\n## Create a Snowflake DataFrame with only SHIFT_SALES and SHIFT_OE with future values (.isNull) for the city \"Vancouver\"."},{"cell_type":"code","execution_count":null,"metadata":{"name":"SnowflakeModelRegistry_ModelInferenceCheck_Code","language":"python","collapsed":false},"outputs":[],"source":"# Check model predictions for holdout data SHIFT_SALES predictions for Location_IDs in Vancouver\ndate_tomorrow_df = oe_df.filter(\n    (F.col(???).isNull())\n    & (F.col(\"shift_oe\") == 1)\n    & (F.col(\"city\") == \"???\")\n)\ndate_tomorrow_df.show()","id":"4b34593d-35ea-4a93-8c46-e8d871aa5a2d"},{"cell_type":"markdown","metadata":{"name":"SnowflakeModelRegistry_ModelInferenceCheck_SiS"},"source":["## Visualize on a Map\n","\n","The red and yellow areas indicate higher predicted sales locations and the green zones indicate lower predicted sales. We will use this insight to ensure that our drivers are parking at the high-value locations. Value: Updated predictions readily available to drive towards our corporate goals.\n","\n"],"id":"88624ec7-88ae-4ce4-b3d7-2322b6e374f0"},{"cell_type":"code","id":"b50ec110-4223-4d70-b46d-4e8622fcf462","metadata":{"language":"python","name":"SnowparkML_VisualizePredictions_SiS_Code","collapsed":false},"outputs":[],"source":"# Pull location predictions into a pandas DataFrame\npredictions_df = result_sdf.to_pandas()\npredictions_df.head()\n\n# Visualize on a map\nst.map(predictions_df)\n","execution_count":null},{"cell_type":"markdown","metadata":{"name":"Part5","collapsed":false},"source":"# Part 5 - Create a SiS application to use predicted outputs\n\nCreate a SiS application for local managers to identify where to place daily food trucks. \n\nSee completed [SiS App](https://app.snowflake.com/sfsenorthamerica/demo72/#/streamlit-apps/HOL.SCHEMA0.NEOTFUP7Z_7K5S04?ref=snowsight_shared) using role=PUBLIC\n\n","id":"fe5ff0fb-e2c7-4aeb-b9e7-805cfae17424"},{"cell_type":"markdown","id":"38d24cfb-06c3-453a-8539-daeb8fac1747","metadata":{"name":"SiSApp","collapsed":false},"source":"# Here is the SiS app code to add to a new Streamlit project. \n# Import Python packages\nimport streamlit as st\nimport pydeck as pdk\nimport numpy as np\n\n\n# Import Snowflake modules\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark import Window\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.registry import Registry\nimport snowflake.ml.modeling.preprocessing as snowmlpp\n\n\n# Set Streamlit page config\nst.set_page_config(\n    page_title=\"Streamlit App: Snowpark 101\", \n    page_icon=\":truck:\",\n    layout=\"wide\",\n)\n\n\n# Add header and a subheader\nst.header(\"Predicted Shift Sales by Location\")\nst.subheader(\"Data-driven recommendations for food truck drivers.\")\n \n\n\n# Connect to Snowflake\n# session = init_connection()\nsession = get_active_session()\n \n# Create input widgets for cities and shift\nwith st.container():\n    col1, col2 = st.columns(2)\n    with col1:\n        # Drop down to select city\n        city = st.selectbox(\n            \"City:\",\n            session.table(\"HOL.SCHEMA0.SHIFT_SALES_V\")\n            .select(\"city\")\n            .distinct()\n            .sort(\"city\"),\n        )\n \n    with col2:\n        # Select AM/PM Shift\n        shift = st.radio(\"Shift:\", (\"AM\", \"PM\"), horizontal=True)\n\n\n    n_trucks = st.selectbox('How many food trucks would you like to schedule today?', np.arange(1,10))\n\n\n    if n_trucks > 1:\n        range = st.slider('What is the minimum distance in kilometers between food trucks?', 0, 20, 1)\n        st.write('You are requesting a minimum distance of ', range, 'km')\n        st.write('Click **:blue[Update]** to get the ', n_trucks, ' highest predicted Shift_Sales food truck locations.')\n    else:\n        st.write('Click **:blue[Update]** to get one food truck location predicted to have the Shift_Sales')\n        \n# Get predictions for city and shift time\ndef get_predictions(city, shift):\n    # Get data and filter by city and shift\n    snowpark_df = session.table(\n        \"HOL.SCHEMA0.SHIFT_SALES_V\"\n    ).filter((F.col(\"shift\") == shift) & (F.col(\"city\") == city))\n \n    # Get rolling average\n    window_by_location_all_days = (\n        Window.partition_by(\"location_id\")\n        .order_by(\"date\")\n        .rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW - 1)\n    )\n \n    snowpark_df = snowpark_df.with_column(\n        \"avg_location_shift_sales\",\n        F.avg(\"shift_sales\").over(window_by_location_all_days),\n    ).cache_result()\n \n    # Get tomorrow's date\n    date_tomorrow = (\n        snowpark_df.filter(F.col(\"shift_sales\").is_null())\n        .select(F.min(\"date\"))\n        .collect()[0][0]\n    )\n \n    # Filter to tomorrow's date\n    snowpark_df = snowpark_df.filter(F.col(\"date\") == date_tomorrow)\n \n    # Impute\n    snowpark_df = snowpark_df.fillna(value=0, subset=[\"avg_location_shift_sales\"])\n\n\n    for colname in snowpark_df.columns:\n        new_colname = str.upper(colname)\n        snowpark_df = snowpark_df.with_column_renamed(colname, new_colname)\n \n    # Encode\n    snowpark_df = snowpark_df.with_column(\"shift_oe\", F.iff(F.col(\"shift\") == \"AM\", 0, 1))\\\n                             .with_column(\"shift_oe\", F.iff(F.col(\"shift\") == \"PM\", 1, 0))\n\n    # Scale\n    mm_target_columns = [\"CITY_POPULATION\"]\n    mm_target_cols_out = [\"CITY_POPULATION_NORM\"]\n    snowml_mms = snowmlpp.MinMaxScaler(input_cols=mm_target_columns, \n                                       output_cols=mm_target_cols_out)\n    snowml_mms.fit(snowpark_df)\n    snowpark_df = snowml_mms.transform(snowpark_df)\n    \n    # Get all features\n    feature_cols = [\"SHIFT_OE\", \n                    \"CITY_POPULATION_NORM\", \n                    \"MONTH\", \n                    \"DAY_OF_WEEK\",\n                    \"LATITUDE\",\n                    \"LONGITUDE\",\n                    \"AVG_LOCATION_SHIFT_SALES\",\n                    \"LOCATION_ID\"]\n\n\n    snowpark_df = snowpark_df.select(feature_cols)\n\n    native_registry = Registry(session=session, database_name=\"HOL\", schema_name=\"SCHEMA0\")\n    model_ver = native_registry.get_model(\"SHIFT_SALES_PREDICTION\").version('v0')\n    result_sdf = model_ver.run(snowpark_df, function_name=\"predict\")\n    return result_sdf\n\n# Update predictions and plot when the \"Update\" button is clicked\nif st.button(\":blue[Update]\"):\n    # Get predictions\n    with st.spinner(\"Getting predictions...\"):\n        predictions_sdf = get_predictions(city, shift)\n        predictions = predictions_sdf.to_pandas()\n \n    # Plot on a map\n    st.subheader(\"Predicted Shift Sales for position\")\n    predictions[\"PRED_SHIFT_SALES\"].clip(0, inplace=True)\n    st.pydeck_chart(\n        pdk.Deck(\n            map_style=None,\n            initial_view_state=pdk.ViewState(\n                latitude=predictions[\"LATITUDE\"][0],\n                longitude=predictions[\"LONGITUDE\"][0],\n                zoom=11,\n                pitch=50,\n            ),\n            layers=[\n                pdk.Layer(\n                    \"HexagonLayer\",\n                    data=predictions,\n                    get_position=\"[LONGITUDE, LATITUDE]\",\n                    radius=200,\n                    elevation_scale=4,\n                    elevation_range=[0, 1000],\n                    pickable=True,\n                    extruded=True,\n                ),\n                pdk.Layer(\n                    \"ScatterplotLayer\",\n                    data=predictions,\n                    get_position=\"[LONGITUDE, LATITUDE]\",\n                    get_color=\"[200, 30, 0, 160]\",\n                    get_radius=200,\n                ),\n            ],\n        )\n    )\n    \n    max_x = predictions.loc[predictions[\"PRED_SHIFT_SALES\"].idxmax()]\n    st.write(\"Maximum Predicted Sales are expected at the following location:\", max_x)\n    #st.dataframe(predictions_sdf)\n    \n    location_id = max_x[\"LOCATION_ID\"]\n    lat = max_x[\"LATITUDE\"]\n    long = max_x[\"LONGITUDE\"]\n\n    st.subheader(\"The following chart is generated using the st_point and st_distance Snowflake Geospatial features\")\n\n    if n_trucks == 1:\n        st.write(\"Have your only food truck positioned at Location ID \", location_id, \" to maximize SHIFT_SALES\")\n    elif n_trucks > 1:\n        best_locations = [location_id]\n        available_locations_sdf = predictions_sdf\n    \n        st_distance = F.function('st_distance')\n        st_point = F.function('st_point')\n    \n        for truck_n in np.arange(0,n_trucks - 1):\n            available_locations_sdf = available_locations_sdf.with_column(\"DISTANCE_TO_TRUCK\", \n                                        st_distance(\n                                            st_point(F.lit(float(long)), F.lit(float(lat))),\n                                            st_point(F.col(\"LONGITUDE\"), F.col(\"LATITUDE\"))\n                                        )/1609\n                                       ).filter(F.col(\"DISTANCE_TO_TRUCK\") >= range/1.609).order_by(\"PRED_SHIFT_SALES\", ascending=False)\n            max_x = available_locations_sdf.limit(1).to_pandas()\n            try:\n                location_id = max_x[\"LOCATION_ID\"].iloc[0]\n                lat = max_x[\"LATITUDE\"].iloc[0]\n                long = max_x[\"LONGITUDE\"].iloc[0]\n            except:\n                break\n            best_locations.append(location_id)\n\n\n        selected_locations = predictions[predictions[\"LOCATION_ID\"].isin(best_locations)]\n        st.map(selected_locations)\n        st.dataframe(selected_locations)"}]}